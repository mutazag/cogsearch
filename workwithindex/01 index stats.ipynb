{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# Variables not used here do not need to be updated in your .env file\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "# credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) if len(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) > 0 else DefaultAzureCredential()\n",
    "credential = DefaultAzureCredential()\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "blob_connection_string = os.environ[\"BLOB_CONNECTION_STRING\"]\n",
    "blob_container_name = os.environ[\"BLOB_CONTAINER_NAME\"]\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_KEY\"] if len(os.environ[\"AZURE_OPENAI_KEY\"]) > 0 else None\n",
    "azure_openai_embedding_deployment = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document_count': 663, 'storage_size': 19386539, 'vector_index_size': 4071272}\n"
     ]
    }
   ],
   "source": [
    "import azure.search as azsearch\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import AnalyzeTextOptions, LexicalAnalyzerName\n",
    "\n",
    "search_index_client = SearchIndexClient(endpoint, credential=credential)\n",
    "print(search_index_client.get_index_statistics(index_name))\n",
    "\n",
    "search_client = search_index_client.get_search_client(index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_results = search_client.search(\n",
    "    search_text=\"eye\",\n",
    "    include_total_count=True,\n",
    "    select=['title', 'parent_id', 'chunk_id', 'chunk'])\n",
    "\n",
    "\n",
    "print(search_results.get_count())\n",
    "print(search_results.get_facets())\n",
    "print(search_results.get_coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"chunk\": \"means building over time a unified, modular, elastic, \\nscalable, secure and well-governed Data Platform that can support a wealth of use cases spanning across the entire \\nanalytical spectrum (descriptive, diagnostic, predictive and prescriptive analytics).   \\n \\nHaving the target Modern Data Platform in place from inception is almost always an utopia and this shouldn\\u2019t \\ndiscourage enterprises but it should actually be an eye opener as often is the use case that drives the realization that \\nthe data estate needs to be modernized in order to be able to innovate with AI/ML.  The good news is that you don\\u2019t \\nhave to spend years to build the target desired Data Platform before embarking on the AI journey because typically a \\nsmall scale reproduction of it, is all it takes to get started. Cloud technologies are important enablers to accelerate \\ntime to value especially when you adopt Platform as a Service (PaaS) capabilities to ingest data from your on-premise \\nsystems of records and from external data sources. \\n \\nIndeed, breaking data silos and storing big data and relevant metadata in a Modern Data Platform enables \\naggregation, enrichment, discovery, exploration, and modeling. The resulting high-quality \\u201cdata operationalization\\u201d \\npractices are foundational to enable those AI operationalization practices where the lifecycle of AI/ML products is \\ncompletely automated, robust, monitored, audited and reproducible. \\nData operationalization (also known as DataOps) is about automating the data lifecycle from raw data into \\ndeployment with the goal to improve quality and reduce cycle time while enabling a digital audit trail through the use \\nof data lineage.  The establishment of DataOps practice represents a critical enabler to ensure that the right data will \\nbe in the hands of the right stakeholders at the right time and with the appropriate level of access control, metadata \\nand governance.\",\n",
      "  \"parent_id\": \"aHR0cHM6Ly9nb2R6aWxsYXN0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2ludGVncmF0ZWR2ZWN0b3ItZG9jcy9FbnRlcnByaXNlJTIwQUklMjBQbGFubmluZy5wZGY1\",\n",
      "  \"title\": \"Enterprise AI Planning.pdf\",\n",
      "  \"chunk_id\": \"8774bc85b15b_aHR0cHM6Ly9nb2R6aWxsYXN0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2ludGVncmF0ZWR2ZWN0b3ItZG9jcy9FbnRlcnByaXNlJTIwQUklMjBQbGFubmluZy5wZGY1_pages_89\",\n",
      "  \"@search.score\": 3.5642433,\n",
      "  \"@search.reranker_score\": null,\n",
      "  \"@search.highlights\": null,\n",
      "  \"@search.captions\": null,\n",
      "  \"@search.document_debug_info\": null,\n",
      "  \"@search.reranker_boosted_score\": null\n",
      "}\n",
      "{\n",
      "  \"chunk\": \"to rapidly turn pilots or Minimal \\nViable Products (MVP) in production deployments and leverage quick and tangible wins to build momentum and \\naccelerate the onboarding of new use cases.  In our experience working with customers, scaling fast means being able \\nto create a sort of AI/ML Factory from day one, with a small scale reproduction of what will likely be the future state in \\nterms of organizational alignment, AI/ML software selection and AI/ML product lifecycle management.  Simplicity, \\nimpact, and risk mitigation are key criteria to make it right at inception and often enterprises leverage tech partners to \\nkick start their AI journey. Often the very first use case is thought to drive efficiencies in mainstream operations with \\nthe objective of demonstrating cost savings as quick win.  \\n \\nBeginning with a specific use case with well-defined Key Performance Indicators (KPIs) and its projected ROI will not \\nonly ensure that value is successfully generated up the line, but will also ensure incorporation of good business \\npractices from the earliest stages. Starting pilots with these aspects in mind from day one, scaling them to production \\nand proving consistent performance and effective management/monitoring over time, is the foundation for a solid \\nand sustainable AI/ML practice.  \\n \\nSuccessful AI/ML projects will rely heavily on strong data foundations.  Organizations should assess whether their data \\nestate is ready to rapidly innovate with AI/ML. This typically means building over time a unified, modular, elastic, \\nscalable, secure and well-governed Data Platform that can support a wealth of use cases spanning across the entire \\nanalytical spectrum (descriptive, diagnostic, predictive and prescriptive analytics).   \\n \\nHaving the target Modern Data Platform in place from inception is almost always an utopia and this shouldn\\u2019t \\ndiscourage enterprises but it should actually be an eye opener as often is the use case that drives the realization that\",\n",
      "  \"parent_id\": \"aHR0cHM6Ly9nb2R6aWxsYXN0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2ludGVncmF0ZWR2ZWN0b3ItZG9jcy9FbnRlcnByaXNlJTIwQUklMjBQbGFubmluZy5wZGY1\",\n",
      "  \"title\": \"Enterprise AI Planning.pdf\",\n",
      "  \"chunk_id\": \"8774bc85b15b_aHR0cHM6Ly9nb2R6aWxsYXN0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2ludGVncmF0ZWR2ZWN0b3ItZG9jcy9FbnRlcnByaXNlJTIwQUklMjBQbGFubmluZy5wZGY1_pages_88\",\n",
      "  \"@search.score\": 2.8299677,\n",
      "  \"@search.reranker_score\": null,\n",
      "  \"@search.highlights\": null,\n",
      "  \"@search.captions\": null,\n",
      "  \"@search.document_debug_info\": null,\n",
      "  \"@search.reranker_boosted_score\": null\n",
      "}\n",
      "{\n",
      "  \"chunk\": \"be covered by your Northwind Health Plus plan. \\n\\nWhen Prior Authorization is Required \\n\\nPrior authorization is required for certain services and treatments such as:  \\n\\n\\u2022 Hospital admissions \\n\\n\\u2022 Inpatient surgery \\n\\n\\u2022 Outpatient surgery \\n\\n\\u2022 Magnetic Resonance Imaging (MRI) \\n\\n\\u2022 Computed Tomography (CT) \\n\\n\\u2022 Radiation Therapy \\n\\n\\u2022 Durable Medical Equipment \\n\\n\\u2022 Physical, Occupational, and Speech Therapy  \\n\\n\\u2022 Home Health Care \\n\\n\\u2022 Infusion Therapy \\n\\n\\u2022 Prosthetics and Orthotics \\n\\n\\u2022 Specialty Drugs  \\n\\n\\n\\nIn certain cases, Northwind Health may require prior authorization even if the service is not \\n\\nlisted above. Your doctor or health care provider should contact Northwind Health to \\n\\ndetermine if prior authorization is required prior to providing care. \\n\\nExceptions to Prior Authorization \\n\\nThere are certain services and treatments that are exempt from prior authorization. These \\n\\ninclude:  \\n\\n\\u2022 Routine office visits \\n\\n\\u2022 Immunizations \\n\\n\\u2022 X-Ray services \\n\\n\\u2022 Emergency services \\n\\n\\u2022 Family planning services \\n\\n\\u2022 Maternity services \\n\\n\\u2022 Services and supplies related to diabetes \\n\\n\\u2022 Preventive care services \\n\\n\\u2022 Mental health and substance abuse services \\n\\n\\u2022 Routine eye exams \\n\\n\\u2022 Routine dental exams \\n\\nIt is important to note that the list of services and treatments that are exempt from prior \\n\\nauthorization is subject to change. Your doctor or health care provider should contact \\n\\nNorthwind Health to determine if prior authorization is required prior to providing care. \\n\\nTips for Obtaining Prior Authorization \\n\\nWhen obtaining prior authorization for a service or treatment, it is important to provide \\n\\nNorthwind Health with all of the necessary information. This includes:  \\n\\n\\u2022 The patient\\u2019s diagnosis \\n\\n\\u2022 The proposed treatment \\n\\n\\u2022 The anticipated duration of the treatment \\n\\n\\u2022 Any other relevant information that may be requested by Northwind Health \\n\\n\\n\\nIt is also important to understand that prior authorization is not a guarantee of payment.\",\n",
      "  \"parent_id\": \"aHR0cHM6Ly9nb2R6aWxsYXN0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2ludGVncmF0ZWR2ZWN0b3ItZG9jcy9Ob3J0aHdpbmRfSGVhbHRoX1BsdXNfQmVuZWZpdHNfRGV0YWlscy5wZGY1\",\n",
      "  \"title\": \"Northwind_Health_Plus_Benefits_Details.pdf\",\n",
      "  \"chunk_id\": \"06b081b451f3_aHR0cHM6Ly9nb2R6aWxsYXN0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2ludGVncmF0ZWR2ZWN0b3ItZG9jcy9Ob3J0aHdpbmRfSGVhbHRoX1BsdXNfQmVuZWZpdHNfRGV0YWlscy5wZGY1_pages_104\",\n",
      "  \"@search.score\": 2.8299677,\n",
      "  \"@search.reranker_score\": null,\n",
      "  \"@search.highlights\": null,\n",
      "  \"@search.captions\": null,\n",
      "  \"@search.document_debug_info\": null,\n",
      "  \"@search.reranker_boosted_score\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for r in search_results:\n",
    "    print(json.dumps(r, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_results.by_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<list_iterator object at 0x0000021A88F71C90>\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.search.documents import SearchClient\n",
    "\n",
    "# search_client = SearchClient(endpoint, index_name, credential=credential)\n",
    "# for r in search_client.search(search_text=\"eye\"):\n",
    "#     print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rapidli 3 10 1\n",
      "turn 11 15 2\n",
      "pilot 16 22 3\n",
      "minim 26 33 5\n",
      "viabl 35 41 6\n",
      "product 42 50 7\n",
      "mvp 52 55 8\n",
      "product 60 70 10\n",
      "deploy 71 82 11\n",
      "leverag 87 95 13\n",
      "quick 96 101 14\n",
      "tangibl 106 114 16\n",
      "win 115 119 17\n",
      "build 123 128 19\n",
      "momentum 129 137 20\n",
      "pilot 142 148 22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "analyse_request = AnalyzeTextOptions(\n",
    "    text = \"to rapidly turn pilots or Minimal \\nViable Products (MVP) in production deployments and leverage quick and tangible wins to build momentum for pilots\",\n",
    "    analyzer_name = LexicalAnalyzerName.EN_LUCENE\n",
    ")\n",
    "\n",
    "analyze_result = search_index_client.analyze_text(index_name, analyse_request)\n",
    "\n",
    "for token in analyze_result.tokens:\n",
    "    print(token.token, token.start_offset, token.end_offset, token.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fields >>>\n",
      "parent_id (Edm.String)\n",
      "title (Edm.String)\n",
      "chunk_id (Edm.String)\n",
      "chunk (Edm.String)\n",
      "vector (Collection(Edm.Single))\n",
      "\n",
      "vector algorithms >>>\n",
      "myHnsw \n",
      "myExhaustiveKnn \n",
      "\n",
      "vector vectorizers >>>\n",
      "myOpenAI \n",
      "{'additional_properties': {'customWebApiParameters': None}, 'name': 'myOpenAI', 'kind': 'azureOpenAI', 'azure_open_ai_parameters': <azure.search.documents.indexes._generated.models._models_py3.AzureOpenAIParameters object at 0x0000021A8AD70350>}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "search_index = search_index_client.get_index(index_name)\n",
    "\n",
    "search_index.as_dict()\n",
    "\n",
    "\n",
    "print('\\nFields >>>')\n",
    "for f in search_index.fields:\n",
    "    print(f\"{f.name} ({f.type})\")\n",
    "\n",
    "\n",
    "search_index_vector_search = search_index.vector_search\n",
    "\n",
    "print('\\nvector algorithms >>>')\n",
    "for algo in search_index_vector_search.algorithms:\n",
    "    print(f\"{algo.name} \")\n",
    "print('\\nvector vectorizers >>>')\n",
    "for vec in search_index_vector_search.vectorizers:\n",
    "    print(f\"{vec.name} \")\n",
    "    print(vec)\n",
    "\n",
    "\n",
    "print(search_index.additional_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
